#算法体系
分词、词性标注、实体识别等，其主要算法结构为基于Bi-LSTM-CRF算法体系，下面对Bi-LSTM-CRF算法体系进行介绍。

    用CRF是为获取全局最优的输出序列，相当于对lstm信息的再利用
    与传统LSTM不同，双向LSTM同时考虑了过去的特征（通过前向过程提取）和未来的特征（通过后向过程提取）。看起来虽然很复杂，但是简单考虑一下，所谓的后向过程相当于将原始序列逆向输入到LSTM中。所以从这个角度来看，双向LSTM相当于两个LSTM，一个正向输入序列，一个反向输入序列，再将两者的输出结合起来作为最终的结果。

双向LSTM 后加softmax层，输出各个label的概率值，那么在加CRF的原因是
    
    首先softmax对输入的预测是独立的。虽然lstm用到上下文的信息，但是输出的label信息并不会用到上一次输出的label的信息，比如在词性分析中、
    输出之前是没有任何影响的，影响主要发生在lstm网络内部。
    那么问题来了。我们知道在nlp领域的基础算法领域比如对词性分析，主要是依据标注的信息的。比如BEMS，具体含义就不说明了。
    在比如对一个字标注为B，那么依据lstm网络是完全又概率输出label为B，这种输出就属于非法标记。因为B后面是不可能在出现
    一个B标注的信息的。而softmax是无法利用这些信息的。
    而在CRF中有转移概率矩阵的，体现一个标记转移到另一个标注的概率，那么此时B到B的概率为0，CRF会考虑到全局最优化的输出 。
    
#
    预测输出标记的得分公式
    S= ∑A + ∑P 其中A表示转移概率矩阵，P表示lstm输出的softmax概率。最终得分才是输出的标记得分。
    比如BB这种A类得分很低，如果P得分很大，也不一定最终S的分数很大。
### 参考网址

    https://blog.csdn.net/bobobe/article/details/80489303

#序列标注算法
### 基于规则
    
    某种规则匹配，比如正则。
### 基于特征模板
    
    序列标注任务需要用到大量的预料库，使用生成模型HMM，判别模型CRF等
    对句子中的各个位置提取特征时，满足条件的特征取值为1，不满足条件的特征取值为0；然后把特征喂给CRF，training阶段建模标签的转移，进而在inference阶段为测试句子的各个位置做标注。
### 深度学习
    
    首先使用词向量方式将高维编码映射到低维稠密的嵌入层。
    然后将句子的词向量序列输入到神经网络，利用神经网络自动提取特征的特性。
    最后sotfmax输出概率分布。
#
    缺点：
    缺点是对每个token打标签的过程中是独立的分类，不能直接利用上文已经预测的标签（只能靠隐状态传递上文信息），
    进而导致预测出的标签序列可能是非法的，例如标签B-PER后面是不可能紧跟着I-LOC的，但Softmax不会利用到这个信息。


### 基于字的LSTM-CRF模型
 
 模型的第一层是 look-up 层，利用预训练或随机初始化的embedding矩阵将句子中的每个字 xi
x
i
 由one-hot向量映射为低维稠密的字向量（character embedding）xi∈Rd
x
i
∈
R
d
 ，d
d
 是embedding的维度。
 #
    模型的第二层是双向LSTM层，自动提取句子特征。将一个句子的各个字的char embedding序列 (x1,x2,...,xn)
(
x
1
,
x
2
,
.
.
.
,
x
n
)
 作为双向LSTM各个时间步的输入，再将正向LSTM输出的隐状态序列 (h1⟶,h2⟶,...,hn⟶)
(
h
1
⟶
,
h
2
⟶
,
.
.
.
,
h
n
⟶
)
 与反向LSTM的 (h1⟵,h2⟵,...,hn⟵)
(
h
1
⟵
,
h
2
⟵
,
.
.
.
,
h
n
⟵
)
 在各个位置输出的隐状态进行按位置拼接 ht=[ht⟶;ht⟵]∈Rm
h
t
=
[
h
t
⟶
;
h
t
⟵
]
∈
R
m
 ，得到完整的隐状态序列

(h1,h2,...,hn)∈Rn×m
(
h
1
,
h
2
,
.
.
.
,
h
n
)
∈
R
n
×
m

在设置dropout后，接入一个线性层，将隐状态向量从 m
m
 维映射到 k
k
 维，k
k
 是标注集的标签数，从而得到自动提取的句子特征，记作矩阵 P=(p1,p2,...,pn)∈Rn×k
P
=
(
p
1
,
p
2
,
.
.
.
,
p
n
)
∈
R
n
×
k
 。
 #
 模型的第三层是CRF层，进行句子级的序列标注。CRF层的参数是一个 (k+2)×(k+2)
(
k
+
2
)
×
(
k
+
2
)
 的矩阵 A
A
 ，Aij
A
i
j
 表示的是从第 i
i
 个标签到第 j
j
 个标签的转移得分，进而在为一个位置进行标注的时候可以利用此前已经标注过的标签，之所以要加2是因为要为句子首部添加一个起始状态以及为句子尾部添加一个终止状态。如果记一个长度等于句子长度的标签序列 y=(y1,y2,...,yn)
y
=
(
y
1
,
y
2
,
.
.
.
,
y
n
)
 ，那么模型对于句子 x
x
 的标签等于 y
y
 的打分为

score(x,y)=∑i=1nPi,yi+∑i=1n+1Ayi−1,yi

### 参考网址
https://www.cnblogs.com/Determined22/p/7238342.html